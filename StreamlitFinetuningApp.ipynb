{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNrsrwQ6jUQxDWNscHwXOBb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxreynolds/Pattern-Cloud/blob/main/StreamlitFinetuningApp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JihJ7XZ2PddX"
      },
      "outputs": [],
      "source": [
        "# Setup code\n",
        "!cd /content/\n",
        "!pip install -q streamlit\n",
        "!git clone https://github.com/huggingface/diffusers.git\n",
        "!pip install -q ./diffusers\n",
        "!pip install -q  -U -r /content/diffusers/examples/text_to_image/requirements.txt\n",
        "!pip install -q wandb\n",
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!nohup /content/cloudflared-linux-amd64 tunnel --url http://localhost:8501 &"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate config default --mixed_precision fp16"
      ],
      "metadata": {
        "id": "GDgEQ7djapZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "QhGH6ifAabXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "v80zt9Nb6CIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making directories for multipage Streamlit applicaiton files and storing user images\n",
        "!mkdir /content/pages\n",
        "!mkdir /content/user_images\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuuhXaihYnFw",
        "outputId": "a8bf8664-0b0b-4c2a-af00-921b562dea60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cloudflared-linux-amd64  diffusers  nohup.out  pages  sample_data  user_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Upload.py\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "import streamlit as st\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def delete_files_in_directory(directory_path):\n",
        "   try:\n",
        "     files = os.listdir(directory_path)\n",
        "     for file in files:\n",
        "       file_path = os.path.join(directory_path, file)\n",
        "       if os.path.isfile(file_path):\n",
        "         os.remove(file_path)\n",
        "   except OSError:\n",
        "     print(\"Error occurred while deleting files.\")\n",
        "\n",
        "image_set = []\n",
        "image_set = st.file_uploader(\"Upload a folder of your training dataset Images:\",\n",
        "    type=[\"png\"], accept_multiple_files=True) # TODO: expand to type=[\"png\",\"jpg\",\"jpeg\"]]... need to update metadata.csv writer code to check for filetype\n",
        "\n",
        "if image_set is not None:\n",
        "    #st.write('Scroll down for next steps')\n",
        "    st.image(image_set, width=128)\n",
        "\n",
        "if st.button('Upload these Images as your training dataset', type=\"primary\", use_container_width=True):\n",
        "    placeholder = st.empty()\n",
        "    placeholder.text(\"Formatting images...\")\n",
        "    for idx, image in enumerate(image_set):\n",
        "      im = Image.open(image)\n",
        "      im.save(f\"/content/user_images/image{idx}.png\")\n",
        "    num_images = len(image_set)\n",
        "    my_label = \"<r4nd0m-l4b3l>\"\n",
        "    with open('/content/user_images/metadata.csv', 'w', newline='') as csvfile:\n",
        "      spamwriter = csv.writer(csvfile, delimiter=' ',\n",
        "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "      spamwriter.writerow(['file_name,text'])\n",
        "      for j in range(num_images):\n",
        "        spamwriter.writerow([f'image{j}.png,'] + [my_label])\n",
        "    placeholder.text('Loading dataset...')\n",
        "    dataset = load_dataset(\"imagefolder\", data_dir=\"/content/user_images\", drop_labels=True)\n",
        "    placeholder.text('Pushing to Hub...')\n",
        "    dataset.push_to_hub(\"MaxReynolds/MyPatternDataset\", private=False)\n",
        "    placeholder.text('Upload complete!')\n",
        "    #removing image files here\n",
        "    delete_files_in_directory('/content/user_images')\n"
      ],
      "metadata": {
        "id": "BabENL7sP2xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/pages/1Train.py\n",
        "import streamlit as st\n",
        "import subprocess\n",
        "\n",
        "# should use this function at some point to remove pre-existing model from directory if it exists\n",
        "def delete_files_in_directory(directory_path):\n",
        "   try:\n",
        "     files = os.listdir(directory_path)\n",
        "     for file in files:\n",
        "       file_path = os.path.join(directory_path, file)\n",
        "       if os.path.isfile(file_path):\n",
        "         os.remove(file_path)\n",
        "   except OSError:\n",
        "     print(\"Error occurred while deleting files.\")\n",
        "\n",
        "num_train_steps = st.slider('Select the number of training steps for your training process:', 0, 450, 250)\n",
        "\n",
        "if st.button('Start training on your dataset', type=\"primary\", use_container_width=True):\n",
        "  st.write('Training...')\n",
        "  subprocess.run([\"accelerate\", \"launch\", \"diffusers/examples/text_to_image/train_text_to_image.py\",\n",
        "    \"--pretrained_model_name_or_path=CompVis/stable-diffusion-v1-4\",\n",
        "    \"--dataset_name=MaxReynolds/MyPatternDataset\",\n",
        "    \"--use_ema\",\n",
        "    \"--resolution=512\",\n",
        "    \"--center_crop\",\n",
        "    \"--random_flip\",\n",
        "    \"--train_batch_size=1\",\n",
        "    \"--gradient_accumulation_steps=4\",\n",
        "    \"--gradient_checkpointing\",\n",
        "    f\"--max_train_steps={num_train_steps}\",\n",
        "    \"--learning_rate=1e-05\",\n",
        "    \"--max_grad_norm=1\",\n",
        "    \"--checkpointing_steps=100000\",\n",
        "    \"--lr_scheduler=constant\",\n",
        "    \"--lr_warmup_steps=0\",\n",
        "    \"--push_to_hub\",\n",
        "    \"--output_dir=MaxReynolds/MyPatternModel\", #Can probably come up for a better name for this directory if not pushing to HF Hub (aka /content/models/)\n",
        "    \"--validation_prompt=,<r4nd0m-l4b3l>\",\n",
        "    \"--report_to=wandb\",\n",
        "    \"--seed=1337\"])\n",
        "  st.write('Training complete!')\n",
        "\n"
      ],
      "metadata": {
        "id": "PV_dnAFVb_DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/pages/2Generate.py\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "if st.button('Generate some images', type=\"primary\", use_container_width=True):\n",
        "  model_path = \"MaxReynolds/MyPatternModel\"\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16)\n",
        "  pipe.to(\"cuda\")\n",
        "  my_prompt = \"<r4nd0m-l4b3l>\"\n",
        "  num_images = 8\n",
        "  # Run inference using above prompt to acquire some number of images\n",
        "  all_images = [pipe(prompt=my_prompt).images[0] for i in range(num_images)]\n",
        "  col1, col2, col3, col4 = st.columns(4)\n",
        "  with col1:\n",
        "    st.image(all_images[0])\n",
        "    st.image(all_images[4])\n",
        "  with col2:\n",
        "    st.image(all_images[1])\n",
        "    st.image(all_images[5])\n",
        "  with col3:\n",
        "    st.image(all_images[2])\n",
        "    st.image(all_images[6])\n",
        "  with col4:\n",
        "    st.image(all_images[3])\n",
        "    st.image(all_images[7])\n"
      ],
      "metadata": {
        "id": "miAe1_GVvu7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/Upload.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "l7A6KetHP04i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -o 'https://.*\\.trycloudflare.com' nohup.out | head -n 1 | xargs -I {} echo \"Your tunnel url {}\""
      ],
      "metadata": {
        "id": "VGBC6VycPwRB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}